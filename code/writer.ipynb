{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile #处理压缩打包文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "#这个类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理 去除符号\n",
    "import re\n",
    "def rm_tags (text): #传递文本\n",
    "    re_tag = re.compile(r'<[^>]+>') #清除标签，符号等  例如<a></a>（因为是从网页上找的影评）\n",
    "    return re_tag.sub('',text)  #只输出内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据\n",
    "import os\n",
    "def read_files(filetype):      #train/test都有\n",
    "    path = \"D:/filp/program/Writer.git/data/imdb/\"  #路径\n",
    "    file_list=[]   #存储读取的所有正负面语料文件名字\n",
    "    \n",
    "    positive_path=path+filetype+\"/pos/\"  #寻找所有正面评价语料\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list+=[positive_path+f]        #将正面评价路径附加到存储列表里\n",
    "\n",
    "    negative_path=path+filetype+\"/neg/\"   #负面\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list+=[negative_path+f]         #负面路径\n",
    "\n",
    "    print('read',filetype,'files:',len(file_list))\n",
    "        \n",
    "    all_labels=([1] * 12500 + [0] * 12500)  #1为正面  0为负面\n",
    "\n",
    "    all_texts = []\n",
    "\n",
    "    for fi in file_list:\n",
    "        with open(fi,encoding='utf8') as file_input:   #分流的方式打开文件\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]  #一次读取一行并清除标签\n",
    "\n",
    "    return all_labels,all_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_train,train_text=read_files(\"train\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_test,test_text=read_files(\"test\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看正面评价第一个\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查是不是1\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看负面评价第一个\n",
    "train_text[12500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查是不是0\n",
    "y_train[12500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先读取所有文章建立字典,限制字典的数量为nb_ words=2000\n",
    "#将文本分解成不同的单元（分为单词，若分成字符没有意义）成为标记\n",
    "#词袋模型（已经淘汰）：分词之后，将标记放到无序的集合中，而不是一个列表或序列，即标记没有特定的顺序\n",
    "#单词转向量:词嵌入(低维度、密集、从数据中学习，、0neHot编码(高维、稀疏、需要编码)\n",
    "\n",
    "#使用Keras的Tokenizer模块实现转换。当我们创建了一个Tokenizer对象后，使用该对象的fit_on_texts()函数，\n",
    "#可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小。使用word_index属性可以看到每次词对应的编码。\n",
    "token = Tokenizer(num_words=2000) \n",
    "token.fit_on_texts(train_text) #分词，之后计算数量再排序，取前num_woeds个作为字典元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer橱性\n",
    "#fit on texts 读取多少文章\n",
    "print(token.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计各个单词出现的频率\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每一篇文章的中的文字转换为一连串的数字\n",
    "x_train_seq = token.texts_to_sequences(train_text)#将多个文档转换为word下标的向量形式\n",
    "x_test_seq  = token.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个单词对应一个数字 表示该单词再词典中的位置\n",
    "print(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#让转换后的数字长度相同#文章内的文字，转换为数字后，每一篇的文章所产生的数字长度都不同，\n",
    "#因为之后需要进行类神经网络的训练，所以每篇文章所产生的数字长度必须相同\n",
    "#以下列程序代码为例max1en=100，所以每一篇文章转换为数字都必须为100\n",
    "x_train = sequence.pad_sequences(x_train_seq,maxlen=100)#让每句数字影评长度相同\n",
    "x_test  = sequence.pad_sequences(x_test_seq, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果文章转换的数字数量大于maxlen, pad_sequences处理后，会切掉前面的数字\n",
    "#因为前面的话往往是铺垫，中后才是表达情感的部分\n",
    "print('before pad_sequences length=',len(x_train_seq[0]))\n",
    "print(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('after pad_sequences length=',len(x_train[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果文章转成数字不足100, pad seguences处理后，前面会加0\n",
    "print('before pad_sequences length=',len(x_train_seq[3]))\n",
    "print(x_train_seq[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('after pad_sequences length=',len(x_train[3]))\n",
    "print(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型 引入嵌入层  Embedding:词嵌入\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import Embedding#从稀疏矩阵到密集矩阵的过程，叫做embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加词嵌入 即单词转向量\n",
    "model.add(Embedding(output_dim=32,    #每个单词转换成向量的长度\n",
    "                    input_dim=2000,   #对应词典大小\n",
    "                    input_length=100))#对应处理后的长度\n",
    "\n",
    "model.add(Dropout(0.2)) #丢弃层，尽可能降低过拟合，每次只选择80%\n",
    "\n",
    "#下面的警告是Tensorflow版本问题  提示有的语句新版本不适用 但是换版本的话程序运行不了 和python有冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())  #添加一个平坦层  把多维向量一维化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=256,activation='relu')) #units :设置神经元数量256个 激活函数是rulu\n",
    "model.add(Dropout(0.2)) #丢弃层，尽可能降低过拟合，每次只选择80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出模型训练结果\n",
    "model.add(Dense(units=1,\n",
    "                activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看模型\n",
    "model.summary()\n",
    "#嵌入层  参数64000=32*2000；\n",
    "#隐藏层  819456=3200（3201）*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化模型 归一化处理\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "#epochs                                                  批次数               80%训练   20%做验证\n",
    "train_history =model.fit(x_train, y_train,batch_size=100,epochs=10,verbose=2,validation_split=0.2)\n",
    "\n",
    "#loss：损失  acc:精度   val_loss验证损失误差   val_acc验证精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history,train,validation):#训练集 和验证集\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'],loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras库老版本中的参数不是accuracy，而是acc\n",
    "#keras库老版本中的参数不是val_accuracy,而是val_acc\n",
    "show_train_history(train_history,'acc','val_acc')   \n",
    "#精度表\n",
    "#蓝色为训练   黄色为验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印损失和误差\n",
    "show_train_history(train_history,'loss','val_loss')\n",
    "#损失表\n",
    "#蓝色为训练   黄色为验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型真正的准确率\n",
    "scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "scores[1]\n",
    "#训练的时候0.9922   评估的时候只有0.808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测概率\n",
    "probility=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印前10个文本的评估结果\n",
    "probility[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印12500到12510文本的评估结果  有的判断有错误\n",
    "for p in probility[12500:12510]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果0或1  验证集\n",
    "predict=model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前10个\n",
    "predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes=predict.reshape(-1)#变成一维的 横向显示\n",
    "predict_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看预测结果   将0，1转换成较为直观的中文结果\n",
    "SentimentDict={1:'正面的',0:'负面的'}\n",
    "def display_test_Sentiment(i):\n",
    "    print(test_text[i])                                 #打印文本\n",
    "    print('标签1abel:',SentimentDict[y_test[i]],        #文本标签\n",
    "          '预测结果:',SentimentDict[predict_classes[i]])#预测结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看正面评价第三个文本的预测结果\n",
    "display_test_Sentiment(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes[12500:12510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看负面评价第五个的预测结果\n",
    "display_test_Sentiment(12504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测新的影评\n",
    "input_text='''\n",
    "this is awesome!!! there is no partnership quite like Errol, and Olivia. there love is genuine! I'm 24, yet this flick is as captivating now as I'm sure it was 60 years ago. Raoul Walsh is an under-rated genius, his direction is so sweeping, so broad, yet so intimate. the last scene between colonel custer (Flynn), and his wife (de havilland), almost brought me to tears (Not easy for a 24yr old guy!!), its so heart-wrenching. there is also a deep Christian message implicit here, the faith Custer has in taking your glory with you, and the trust, and fidelity of his wife to the extent of letting him go, in order that he fulfils his moral duty to protect the innocent civilians from certain massacre. there is no movie that deals with these issues quite like this. a must-see for anyone who wants to look at this defining moment in American, and military history, from the inside. patriotic, for all the right reasons. i knew Errol Flynn was a star, and De havilland was a screen legend-this only confirms my suspicions that they are among the very greatest!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面的工作也可以通过def定义为一个函数一步处理完毕\n",
    "#需要对输入的文本做预处理\n",
    "input_seq = token.texts_to_sequences([input_text]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本数字化长度\n",
    "len(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#截取为长度100的数据\n",
    "pad_input_seq = sequence.pad_sequences(input_seq , maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pad_input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result=model.predict_classes(pad_input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDict[predict_result[0][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
